# logger options
description: "Softmax loss classification"
# dataset options

task: 'T&E' # 'T&E' 
# if 'I&O' is chosen, the code will print the label with confidence of this image
# if 'T&E' then you can use the configs down with training and evaluation process.

method:
    type: 'deep_learning' 

# I&O: provide an image or a folder of images in image_path, the models used are defined in path_to_models:
image_path: './datasets/dataset/stm_data/test_new'

# 'T&E'
dataset:
    name : 'stm_data' # 
    extract_to: ''
    user_name: 'ihabasaad'
    key: '3db5147e652da8d638a22f3ceda90752'

net: 
    arch : 'resnext50' # 'resnet50' # 'resnet18', 'resnet34', 'resnet50', 'resnet1001', 'resnet101','resnet152': download pretrained weights
    # 'inception': from scratch, not added yet
    # 'efficientnet', 'efficientnet_b0','efficientnet_b1','efficientnet_b2','efficientnet_b3','efficientnet_b4','efficientnet_b5','efficientnet_b6','efficientnet_b7'
    # 'cusnet'
    # 'vit' vision transformers:
    # Check the data images sizes ??? important
    height : 299 # 224 # 256 
    width :  299 # 224 # 128 

# training
training:
    epochs : 15
    batch_size: 16 # 
    workers : 2 # 
    features: 256 # 
    dropout: 0.3
    lr: 0.01 # learning rate of new parameters, for pretrained, parameters it is 10 times smaller than this
    momentum: 0.9
    weight_decay: 0.0005
    opt: SGD # not added yet
    loss: softmax_loss # not added yet # https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook
    metrics : ['acc'] # not added yet
    num_instances: 4 # each minibatch consist of "(batch_size // num_instances) identities, and "each identity has num_instances instances"
    margin: 0.5 # for triplet loss

training_configs:
    cv: True # cross validation
    folds: 5
    combine_trainval: False # if True train and val sets together for training, and val set alone for validataion.
    resume : '' # put your model path here to start training from it.
    evaluate : False # by default False (training mode), if True, evaluation only, and the path to model will be from 'resume' variable
    predict: False
    path_to_models: ["1Qajyh0DLb4PlEQ2W5H29LiSoyRPeR8CL&confirm=t",
                "1zbRMjmk7I0V4Wupla7g3d-eVxbXRg4LA&confirm=t",
                "1Au0w6ZxY-0BRN631vZFtmWya6UzDiaUy&confirm=t",
                "1iT-2TdDQYDbka9lf6-S40oNxVSm-Co-O&confirm=t",
                "1YgOZ_fQsGFqLHqBZwYq_6lzimqvynZsP&confirm=t",
                "1y3_QidklP12vYe1C3Sdl1RrS0DNDRN0Q&confirm=t",
                "1wuBa5R5DiPCo-96-euXQlckuKgFE9gln&confirm=t",
                "1aUMvJKEfya-u1ihM0FjIVIMqPGilHNJq&confirm=t",
                "1QdtciWd4VHyKYb9g30O-HayTvYDmbffO&confirm=t",
                "1zF8f-0G1O98YVP5CaHx-SsphNKEJJDyg&confirm=t"]
    start_save: 0 # start saving checkpoints after specific epoch
    seed: 1
    print_freq: 1 # print to stdout each print_freq epochs
    val_split: 0.1 # val/(train_val)
    test_split: 0.01 # test/(test+train_val)
    split : 0 # The index of data split. Default: 0
    dest_path: '../content/datasets/' # not used yet

metric_learning:
    dist_metric: 'euclidean' # ['euclidean', 'kissme']

logging:
    working_dir: '/datachallenge'
    data_dir: './datasets/dataset' # + dataset_name : where to store downloaded data
    logs_dir: './logs/test_loss' # where to save training and testing logs
